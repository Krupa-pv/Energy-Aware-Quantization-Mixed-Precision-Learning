\documentclass{article}

% NeurIPS 2024 style
\usepackage[final]{neurips_2024}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}

% Custom colors
\definecolor{pareto}{RGB}{46, 134, 193}
\definecolor{highlight}{RGB}{231, 76, 60}

\title{Energy-Aware Quantization and Mixed-Precision Learning for Vision Models}

\author{
  Salma Bhar \\
  \texttt{sxb1283@psu.edu} \\
  \And
  Wiam Skakri \\
  \texttt{wxs428@psu.edu} \\
  \And
  Krupa Venkatesan \\
  \texttt{kxv178@psu.edu} \\
}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
As deep learning models scale in size and complexity, their energy consumption during inference has become a critical concern for deployment on edge devices and large-scale data centers. In this work, we investigate post-training quantization (PTQ) and sensitivity-based mixed-precision inference as lightweight approaches for improving the energy efficiency of vision models. We evaluate our methods on ResNet-18 (CIFAR-100) and DeiT-Tiny (ImageNet), applying uniform quantization at 8-bit, 6-bit, and 4-bit precisions, as well as a mixed-precision scheme that assigns bitwidths based on layer-wise sensitivity analysis. Using an analytical energy model that accounts for both MAC operations and memory access, we demonstrate energy reductions of up to 88\% at 4-bit precision. Our mixed-precision configuration achieves 85.7\% energy reduction on ResNet-18 while maintaining 99\% of baseline accuracy. We observe that DeiT-Tiny exhibits remarkable robustness to quantization, maintaining 85.01\% accuracy at 6-bit precision with only a 0.33 percentage point drop from FP32. Our findings highlight the potential of training-free quantization approaches for energy-efficient deployment, while also revealing the limitations of aggressive quantization without additional optimization techniques.
\end{abstract}

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

The rapid advancement of deep learning has led to increasingly large and powerful models, particularly in computer vision. Vision Transformers (ViTs)~\cite{dosovitskiy2020vit} and their efficient variants like DeiT~\cite{touvron2021deit} have achieved state-of-the-art performance on image classification tasks, but at the cost of significant computational and memory requirements. This poses a fundamental challenge for deploying these models on resource-constrained edge devices (mobile phones, IoT sensors, autonomous vehicles) and in large-scale inference services where energy consumption directly impacts operational costs and environmental footprint.

The total energy consumption of neural network inference can be decomposed into two primary components~\cite{horowitz2014energy}:
\begin{equation}
E = N_{\text{MAC}} \cdot E_{\text{MAC}} + N_{\text{mem}} \cdot E_{\text{DRAM}}
\label{eq:energy_basic}
\end{equation}
where $N_{\text{MAC}}$ is the number of multiply-accumulate operations, $E_{\text{MAC}}$ is the energy per MAC, $N_{\text{mem}}$ is the number of memory accesses, and $E_{\text{DRAM}}$ is the energy per memory access. Critically, memory access dominates energy consumption in modern systems—a 32-bit DRAM access consumes approximately 200× more energy than a 32-bit floating-point multiply~\cite{horowitz2014energy}.

\textbf{Quantization} addresses this challenge by reducing the numerical precision of weights and activations, which decreases both memory footprint and computational cost. Post-training quantization (PTQ) is particularly attractive as it requires no retraining—only a small calibration dataset is needed to determine quantization parameters.

\textbf{Our Contributions:}
\begin{enumerate}
    \item We implement and evaluate PTQ at multiple precision levels (8/6/4-bit) for both CNN (ResNet-18) and Transformer (DeiT-Tiny) architectures.
    \item We develop a sensitivity-based mixed-precision assignment strategy that allocates bitwidths according to layer-wise quantization sensitivity.
    \item We provide an analytical energy model to estimate energy savings and construct Pareto frontiers characterizing the accuracy-energy trade-off.
    \item We analyze the differential quantization robustness of CNNs versus Vision Transformers, observing that transformers maintain accuracy under aggressive quantization better than CNNs.
\end{enumerate}

%==============================================================================
% RELATED WORK
%==============================================================================
\section{Related Work}

\subsection{Post-Training Quantization}

Post-training quantization has emerged as a practical approach for model compression without the computational overhead of quantization-aware training (QAT). Early work focused on CNNs, establishing techniques like symmetric and asymmetric quantization, per-tensor and per-channel scaling, and calibration methods for determining optimal quantization ranges~\cite{jacob2018quantization, krishnamoorthi2018whitepaper}.

More recently, PTQ methods have been adapted for Transformer architectures. Liu et al.~\cite{liu2021ptqvit} identified that Vision Transformers pose unique challenges for quantization due to the softmax attention mechanism, where relative rankings of attention scores must be preserved. They introduced ranking-aware quantization objectives and mixed-precision assignment based on feature diversity measured through nuclear norms of attention maps.

\subsection{Mixed-Precision Quantization}

Mixed-precision quantization assigns different bitwidths to different layers based on their sensitivity to quantization error. HAQ~\cite{wang2019haq} uses reinforcement learning to search for optimal bitwidth assignments. HAWQ~\cite{dong2019hawq} employs second-order Hessian information to guide precision allocation. More recently, methods like BRECQ~\cite{li2021brecq} and QDrop~\cite{wei2022qdrop} have achieved strong results by carefully reconstructing block-wise outputs during calibration.

\subsection{Energy-Aware Neural Networks}

Spingarn et al.~\cite{spingarn2023energy} provided key insights into the energy consumption of low-precision neural networks. They observed that (1) signed arithmetic (two's complement) causes significant bit toggling and power consumption, and (2) multiplier power is dominated by the larger input bitwidth. Their work motivates energy-aware design choices beyond simple accuracy-compression trade-offs.

Our work builds upon these foundations, implementing sensitivity-based mixed-precision PTQ with an analytical energy model to guide and evaluate our quantization decisions.

%==============================================================================
% METHODOLOGY
%==============================================================================
\section{Methodology}

\subsection{Problem Formulation}

Given a pretrained model $f_\theta$ with full-precision (FP32) weights $\theta$, our goal is to find quantized weights $\hat{\theta}$ that minimize accuracy degradation while maximizing energy efficiency:
\begin{equation}
\min_{\hat{\theta}} \mathcal{L}(\hat{\theta}) \quad \text{s.t.} \quad E(\hat{\theta}) \leq E_{\text{budget}}
\end{equation}
where $\mathcal{L}$ is the task loss and $E$ is the estimated inference energy.

\subsection{Uniform Symmetric Quantization}

We employ uniform symmetric quantization, mapping floating-point values to a discrete set of integers. For a tensor $x$ with bitwidth $b$:
\begin{equation}
\hat{x} = s \cdot \text{clamp}\left(\text{round}\left(\frac{x}{s}\right), -2^{b-1}, 2^{b-1}-1\right)
\end{equation}
where the scale factor $s$ is determined during calibration:
\begin{equation}
s = \frac{\max(|x|)}{2^{b-1} - 1}
\end{equation}

We apply \textbf{per-channel quantization} for weights (separate scale per output channel) and \textbf{per-tensor quantization} for activations, following best practices~\cite{krishnamoorthi2018whitepaper}.

\subsection{Sensitivity Analysis}

To enable mixed-precision assignment, we measure the quantization sensitivity of each layer $l$ as the L2 distance between full-precision and quantized outputs:
\begin{equation}
S_l = \|y_l^{\text{full}} - y_l^{\text{quant}}\|_2
\label{eq:sensitivity}
\end{equation}
This metric is computed during a calibration pass using a small subset of training data. Layers with high sensitivity receive more bits to preserve accuracy, while less sensitive layers are more aggressively quantized.

\subsection{Mixed-Precision Assignment}

Based on the sensitivity analysis, we assign bitwidths using percentile thresholds:
\begin{equation}
b_l = \begin{cases}
8 & \text{if } S_l \geq P_{75}(S) \\
6 & \text{if } P_{25}(S) \leq S_l < P_{75}(S) \\
4 & \text{if } S_l < P_{25}(S)
\end{cases}
\end{equation}
where $P_k(S)$ denotes the $k$-th percentile of sensitivities across all layers. This approach allocates 8-bit precision to the top 25\% most sensitive layers, 4-bit to the bottom 25\%, and 6-bit to the middle 50\%.

\subsection{Energy Model}

We use an analytical energy model that captures the key factors affecting inference energy consumption:
\begin{equation}
E = \sum_l N_{\text{MAC}}^{(l)} \cdot E_{\text{MAC}}(b_l) + \sum_l N_{\text{mem}}^{(l)} \cdot E_{\text{DRAM}}(b_l)
\label{eq:energy_model}
\end{equation}

Based on hardware studies~\cite{horowitz2014energy, spingarn2023energy}, we model:
\begin{itemize}
    \item \textbf{MAC energy}: $E_{\text{MAC}}(b) \propto b^2$ — energy scales quadratically with bitwidth due to multiplier area scaling.
    \item \textbf{Memory energy}: $E_{\text{DRAM}}(b) \propto b$ — energy scales linearly with bitwidth due to data transfer volume.
\end{itemize}

For our baseline configuration, we use a MAC-to-memory energy ratio of 1:200, reflecting the dominance of memory access energy in modern systems.

\subsection{Model Architectures}

We evaluate two representative architectures:

\textbf{ResNet-18}~\cite{he2016resnet}: A convolutional neural network with 11.7M parameters, trained on CIFAR-100. ResNet's residual connections and batch normalization make it amenable to quantization, though the skip connections can amplify quantization errors across layers.

\textbf{DeiT-Tiny}~\cite{touvron2021deit}: A data-efficient Vision Transformer with 5.7M parameters, pretrained on ImageNet. DeiT uses the standard Transformer architecture with self-attention, which introduces unique quantization challenges due to the softmax operation's sensitivity to input magnitude changes.

%==============================================================================
% EXPERIMENTS
%==============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\textbf{Datasets:} We use CIFAR-100 (100 classes, 32×32 images) for ResNet-18 and ImageNet-1K (1000 classes, 224×224 images) for DeiT-Tiny. For calibration, we use 256 randomly sampled training images. For evaluation, we use the full CIFAR-100 test set (10,000 images) and a 10,000-image subset of ImageNet validation.

\textbf{Implementation:} All experiments are conducted in PyTorch. We use the \texttt{timm} library~\cite{rw2019timm} for DeiT-Tiny pretrained weights. Quantization is applied to all linear and convolutional layers, excluding the first and last layers following common practice.

\textbf{Baselines:} We compare against FP32 (full precision), 8-bit uniform PTQ, 6-bit uniform PTQ, and 4-bit uniform PTQ.

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Quantization results for ResNet-18 on CIFAR-100. Energy is reported relative to FP32 baseline.}
\label{tab:resnet_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Config} & \textbf{Accuracy (\%)} & \textbf{$\Delta$ Acc.} & \textbf{Rel. Energy} & \textbf{Energy Savings} \\
\midrule
FP32 (baseline) & 61.44 & — & 1.00 & — \\
8-bit PTQ & 61.32 & -0.12 & 0.238 & 76.2\% \\
6-bit PTQ & 60.42 & -1.02 & 0.178 & 82.2\% \\
4-bit PTQ & 38.04 & -23.40 & 0.118 & 88.2\% \\
\rowcolor{pareto!20}
\textbf{Mixed (Ours)} & \textbf{60.80} & \textbf{-0.64} & \textbf{0.143} & \textbf{85.7\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Quantization results for DeiT-Tiny on ImageNet (10K validation subset).}
\label{tab:deit_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Config} & \textbf{Accuracy (\%)} & \textbf{$\Delta$ Acc.} & \textbf{Rel. Energy} & \textbf{Energy Savings} \\
\midrule
FP32 (baseline) & 85.34 & — & 1.00 & — \\
8-bit PTQ & 85.10 & -0.24 & 0.241 & 75.9\% \\
6-bit PTQ & 85.01 & -0.33 & 0.180 & 82.0\% \\
4-bit PTQ & 83.04 & -2.30 & 0.120 & 88.0\% \\
\rowcolor{pareto!20}
\textbf{Mixed (Ours)} & \textbf{85.05} & \textbf{-0.29} & \textbf{0.191} & \textbf{80.9\%} \\
\bottomrule
\end{tabular}
\end{table}

Tables~\ref{tab:resnet_results} and~\ref{tab:deit_results} present our main quantization results. Several key observations emerge:

\textbf{8-bit quantization is nearly lossless.} Both models maintain near-baseline accuracy with 8-bit PTQ (−0.12\% for ResNet-18, −0.24\% for DeiT-Tiny) while achieving approximately 76\% energy reduction.

\textbf{DeiT-Tiny is remarkably robust to quantization.} Even at 4-bit precision, DeiT-Tiny loses only 2.3 percentage points, compared to ResNet-18's catastrophic 23.4 point drop. This suggests that the self-attention mechanism and layer normalization in transformers provide inherent regularization that improves quantization tolerance.

\textbf{Mixed-precision achieves the best accuracy-energy trade-off.} Our sensitivity-based mixed-precision configuration achieves 85.7\% energy reduction for ResNet-18 with only 0.64\% accuracy loss—substantially better than 4-bit PTQ which sacrifices 23.4\% accuracy for an additional 2.5\% energy savings.

\subsection{Energy Breakdown Analysis}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/resnet_energy_breakdown.png}
    \caption{ResNet-18 (CIFAR-100)}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/deit_energy_breakdown.png}
    \caption{DeiT-Tiny (ImageNet)}
\end{subfigure}
\caption{Energy breakdown by component (MAC vs. memory) across precision configurations. Memory access dominates energy consumption in all cases, accounting for 93-99\% of total energy.}
\label{fig:energy_breakdown}
\end{figure}

Figure~\ref{fig:energy_breakdown} shows the energy breakdown between MAC operations and memory access. Memory energy dominates in all configurations, accounting for 93-99\% of total energy. This validates our energy model's assumption of memory-dominated inference and explains why even modest bitwidth reductions yield substantial energy savings—reducing from 32-bit to 8-bit quarters the memory transfer volume.

\subsection{Pareto Frontier Analysis}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/resnet_pareto.png}
    \caption{ResNet-18 (CIFAR-100)}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/deit_pareto.png}
    \caption{DeiT-Tiny (ImageNet)}
\end{subfigure}
\caption{Pareto frontier showing accuracy vs. relative energy. Mixed-precision configurations achieve favorable trade-offs, particularly for ResNet-18 where 4-bit PTQ causes severe accuracy degradation.}
\label{fig:pareto}
\end{figure}

Figure~\ref{fig:pareto} presents the Pareto frontiers for both models. For ResNet-18, 8-bit and mixed-precision configurations dominate the Pareto frontier, while 4-bit falls significantly below due to accuracy collapse. For DeiT-Tiny, all configurations remain relatively close to the Pareto frontier, with mixed-precision offering a balanced trade-off.

\subsection{Mixed-Precision Bitwidth Distribution}

\begin{table}[t]
\centering
\caption{Bitwidth distribution in mixed-precision configurations.}
\label{tab:bitwidth_dist}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{4-bit Layers} & \textbf{6-bit Layers} & \textbf{8-bit Layers} \\
\midrule
ResNet-18 & 6 (28.6\%) & 9 (42.9\%) & 6 (28.6\%) \\
DeiT-Tiny & 13 (26.0\%) & 24 (48.0\%) & 13 (26.0\%) \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:bitwidth_dist} shows the bitwidth distribution resulting from our sensitivity-based assignment. Both models follow the expected percentile-based allocation, with approximately 25\% of layers at 4-bit and 8-bit, and 50\% at 6-bit.

%==============================================================================
% DISCUSSION
%==============================================================================
\section{Discussion}

\subsection{CNN vs. Transformer Quantization Robustness}

A surprising finding is the stark difference in quantization robustness between ResNet-18 and DeiT-Tiny. While ResNet-18's accuracy collapses at 4-bit (38.04\% vs. 61.44\% baseline), DeiT-Tiny maintains competitive performance (83.04\% vs. 85.34\% baseline). We hypothesize several factors contribute to this:

\begin{enumerate}
    \item \textbf{Softmax normalization}: The attention softmax normalizes scores to sum to 1, which may reduce sensitivity to absolute magnitude errors introduced by quantization.
    \item \textbf{Layer normalization}: Unlike batch normalization in CNNs, layer normalization operates within each sample, potentially providing more stable statistics for quantized activations.
    \item \textbf{Redundancy in attention}: The multi-head attention mechanism distributes information across multiple heads, providing redundancy that may compensate for quantization errors in individual heads.
\end{enumerate}

\subsection{Limitations}

Our work has several important limitations:

\textbf{Theoretical energy model.} Our energy estimates use analytical approximations rather than measurements from actual hardware. The MAC-to-memory energy ratio (1:200) and scaling laws ($E_{\text{MAC}} \propto b^2$, $E_{\text{DRAM}} \propto b$) are based on literature values and may not accurately reflect specific target platforms.

\textbf{Limited baseline comparisons.} We do not compare against state-of-the-art PTQ methods such as GPTQ~\cite{frantar2022gptq}, AWQ~\cite{lin2023awq}, or SmoothQuant~\cite{xiao2022smoothquant}, which have shown strong results on transformer quantization.

\textbf{Evaluation scale.} DeiT-Tiny was evaluated on a 10,000-image subset of ImageNet rather than the full 50,000-image validation set. Additionally, our ResNet-18 baseline (61.44\%) is below typical reported accuracies (~75-78\%) for well-tuned models on CIFAR-100.

\textbf{No hardware validation.} We report theoretical energy savings but do not deploy quantized models on actual edge hardware to measure real latency and power consumption.

%==============================================================================
% CONCLUSION AND FUTURE WORK
%==============================================================================
\section{Conclusion and Future Work}

We have presented a comprehensive study of post-training quantization and sensitivity-based mixed-precision inference for energy-efficient vision models. Our key findings include:

\begin{itemize}
    \item PTQ at 8-bit precision achieves near-lossless accuracy with approximately 76\% energy reduction for both CNNs and Transformers.
    \item DeiT-Tiny exhibits remarkable robustness to aggressive quantization, maintaining 85.01\% accuracy at 6-bit and 83.04\% at 4-bit.
    \item Mixed-precision assignment based on layer sensitivity provides favorable accuracy-energy trade-offs, achieving 85.7\% energy reduction with minimal accuracy loss.
    \item Memory access dominates inference energy, making bitwidth reduction particularly effective for efficiency gains.
\end{itemize}

\textbf{Future directions} include: (1) deploying quantized models on edge hardware (NVIDIA Jetson, mobile NPUs) for real energy measurements; (2) comparing against state-of-the-art PTQ methods; (3) exploring quantization-aware training for recovering accuracy at aggressive compression levels; (4) investigating learned step sizes and asymmetric quantization for improved 4-bit performance; and (5) extending to other domains such as object detection and semantic segmentation.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{unsrtnat}
\bibliography{references}

% Fallback if BibTeX not available
\begin{thebibliography}{20}

\bibitem[Dosovitskiy et al.(2020)]{dosovitskiy2020vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... \& Houlsby, N. (2020).
An image is worth 16x16 words: Transformers for image recognition at scale.
\textit{arXiv preprint arXiv:2010.11929}.

\bibitem[Touvron et al.(2021)]{touvron2021deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., \& Jégou, H. (2021).
Training data-efficient image transformers \& distillation through attention.
\textit{International Conference on Machine Learning (ICML)}.

\bibitem[Horowitz(2014)]{horowitz2014energy}
Horowitz, M. (2014).
Computing's energy problem (and what we can do about it).
\textit{IEEE International Solid-State Circuits Conference (ISSCC)}.

\bibitem[Liu et al.(2021)]{liu2021ptqvit}
Liu, Z., Wang, Y., Han, K., Ma, S., \& Gao, W. (2021).
Post-training quantization for vision transformer.
\textit{Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Spingarn et al.(2023)]{spingarn2023energy}
Spingarn-Eliezer, N., Banner, R., Hoffer, E., Ben-Yaakov, H., \& Michaeli, T. (2023).
Energy awareness in low precision neural networks.
\textit{International Conference on Learning Representations (ICLR)}.

\bibitem[Jacob et al.(2018)]{jacob2018quantization}
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., ... \& Kalenichenko, D. (2018).
Quantization and training of neural networks for efficient integer-arithmetic-only inference.
\textit{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}.

\bibitem[Krishnamoorthi(2018)]{krishnamoorthi2018whitepaper}
Krishnamoorthi, R. (2018).
Quantizing deep convolutional networks for efficient inference.
\textit{arXiv preprint arXiv:1806.08342}.

\bibitem[Wang et al.(2019)]{wang2019haq}
Wang, K., Liu, Z., Lin, Y., Lin, J., \& Han, S. (2019).
HAQ: Hardware-aware automated quantization with mixed precision.
\textit{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}.

\bibitem[Dong et al.(2019)]{dong2019hawq}
Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., \& Keutzer, K. (2019).
HAWQ: Hessian aware quantization of neural networks with mixed-precision.
\textit{IEEE International Conference on Computer Vision (ICCV)}.

\bibitem[Li et al.(2021)]{li2021brecq}
Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., ... \& Yu, F. (2021).
BRECQ: Pushing the limit of post-training quantization by block reconstruction.
\textit{International Conference on Learning Representations (ICLR)}.

\bibitem[Wei et al.(2022)]{wei2022qdrop}
Wei, X., Gong, R., Li, Y., Liu, X., \& Yu, F. (2022).
QDrop: Randomly dropping quantization for extremely low-bit post-training quantization.
\textit{International Conference on Learning Representations (ICLR)}.

\bibitem[He et al.(2016)]{he2016resnet}
He, K., Zhang, X., Ren, S., \& Sun, J. (2016).
Deep residual learning for image recognition.
\textit{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R. (2019).
PyTorch image models.
\url{https://github.com/rwightman/pytorch-image-models}.

\bibitem[Frantar et al.(2022)]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., \& Alistarh, D. (2022).
GPTQ: Accurate post-training quantization for generative pre-trained transformers.
\textit{arXiv preprint arXiv:2210.17323}.

\bibitem[Lin et al.(2023)]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., \& Han, S. (2023).
AWQ: Activation-aware weight quantization for LLM compression and acceleration.
\textit{arXiv preprint arXiv:2306.00978}.

\bibitem[Xiao et al.(2022)]{xiao2022smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., \& Han, S. (2022).
SmoothQuant: Accurate and efficient post-training quantization for large language models.
\textit{arXiv preprint arXiv:2211.10438}.

\end{thebibliography}

%==============================================================================
% APPENDIX (Optional)
%==============================================================================
\appendix
\section{Additional Results}

\subsection{Per-Layer Sensitivity Analysis}

Figure~\ref{fig:sensitivity} shows the per-layer sensitivity distribution for both models. We observe that sensitivity varies significantly across layers, with early convolutional layers and final classification layers typically showing higher sensitivity than intermediate layers.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/sensitivity.png}
\caption{Per-layer sensitivity distribution showing the variation in quantization sensitivity across model layers. Sensitivity is measured as the L2 distance between full-precision and quantized layer outputs.}
\label{fig:sensitivity}
\end{figure}

\subsection{Implementation Details}

\textbf{Calibration procedure:} We perform calibration using 256 randomly sampled images from the training set. For each layer, we compute the maximum absolute value of weights/activations over the calibration set to determine the quantization scale.

\textbf{Quantization scheme:} We use symmetric uniform quantization with per-channel scales for weights and per-tensor scales for activations. The first and last layers are kept at 8-bit precision to preserve input/output fidelity.

\end{document}

